{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Text Summarization with Generative Models on Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/text_summarization.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/text_summarization.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/text_summarization.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "Text summarization produces a concise and fluent summary of a longer text document. There are two main text summarization types: extractive and abstractive. Extractive summarization involves selecting critical sentences from the original text and combining them to form a summary. Abstractive summarization involves generating new sentences representing the original text's main points. In this notebook, you go through a few examples of how large language models can help with generating summaries based on text.\n",
    "\n",
    "Learn more about text summarization in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/summarization-prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you will learn how to use generative models to summarize information from text by working through the following examples:\n",
    "- Transcript summarization\n",
    "- Summarizing text into bullet points\n",
    "- Dialogue summarization with to-dos\n",
    "- Hashtag tokenization\n",
    "- Title & heading generation\n",
    "\n",
    "You also learn how to evaluate model-generated summaries by comparing to human-created summaries using ROUGE as an evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6d865e68adb"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bs9TZo0GJKCR"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5AEr0lkLKD"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "148dd6321946",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform google-cloud-translate --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLVWFKFwkLKE"
   },
   "source": [
    "**Colab only:** Uncomment the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Hsqwn4hkLKE"
   },
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe7OuYuGkLKF"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](#TODO).\n",
    "* If you are using **local Jupyter**, check out the setup instructions [here](#TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9Gx2SAZkLKF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Let's start by importing the libraries that we will need for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colab only:** Uncomment the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "### Import models\n",
    "\n",
    "Here we load the pre-trained text generation model called `text-bison@001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the translation wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate\n",
    "\n",
    "project_id = !gcloud config list project\n",
    "project_id = project_id[1].split('=')[1].strip()\n",
    "parent = f'projects/' + project_id\n",
    "\n",
    "\n",
    "def traduza(texto, idioma_destino):\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    response = client.translate_text(\n",
    "        parent=parent,\n",
    "        contents=[texto],\n",
    "        target_language_code=idioma_destino,\n",
    "        mime_type=\"text/plain\"\n",
    "    )\n",
    "\n",
    "    return response.translations[0].translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mu1UAhoTKn51"
   },
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgZvJeBpJKCS"
   },
   "source": [
    "### Transcript summarization\n",
    "\n",
    "In this first example, you summarize a piece of text on quantum computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UA2NjngeJKCS"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Forneça um resumo muito curto, não mais do que três frases, para o seguinte artigo:\n",
    "\n",
    "Nossos computadores quânticos funcionam manipulando qubits de uma forma orquestrada que chamamos de algoritmos quânticos.\n",
    "O desafio é que os qubits são tão sensíveis que até mesmo a luz difusa pode causar erros de cálculo – e o problema piora à medida que os computadores quânticos crescem.\n",
    "Isso tem consequências significativas, pois os melhores algoritmos quânticos que conhecemos para executar aplicativos úteis exigem que as taxas de erro de nossos qubits sejam muito menores do que as que temos hoje.\n",
    "Para preencher essa lacuna, precisaremos de correção de erro quântica.\n",
    "A correção de erros quânticos protege as informações codificando-as em vários qubits físicos para formar um “qubit lógico” e acredita-se que seja a única maneira de produzir um computador quântico de grande escala com taxas de erro baixas o suficiente para cálculos úteis.\n",
    "Em vez de calcular nos próprios qubits individuais, calcularemos em qubits lógicos. Ao codificar números maiores de qubits físicos em nosso processador quântico em um qubit lógico, esperamos reduzir as taxas de erro para permitir algoritmos quânticos úteis.\n",
    "\n",
    "Resumo:\n",
    "\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aade04b2e86a"
   },
   "source": [
    "Instead of a summary, we can ask for a TL;DR (\"too long; didn't read\"). You can compare the differences between the outputs generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c0c0f3dfe10"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Forneça um TL;DR para o seguinte artigo:\n",
    "\n",
    "Nossos computadores quânticos funcionam manipulando qubits de uma forma orquestrada que chamamos de algoritmos quânticos.\n",
    "O desafio é que os qubits são tão sensíveis que até mesmo a luz difusa pode causar erros de cálculo – e o problema piora à medida que os computadores quânticos crescem.\n",
    "Isso tem consequências significativas, pois os melhores algoritmos quânticos que conhecemos para executar aplicativos úteis exigem que as taxas de erro de nossos qubits sejam muito menores do que as que temos hoje.\n",
    "Para preencher essa lacuna, precisaremos de correção de erro quântica.\n",
    "A correção de erros quânticos protege as informações codificando-as em vários qubits físicos para formar um “qubit lógico” e acredita-se que seja a única maneira de produzir um computador quântico de grande escala com taxas de erro baixas o suficiente para cálculos úteis.\n",
    "Em vez de calcular nos próprios qubits individuais, calcularemos em qubits lógicos. Ao codificar números maiores de qubits físicos em nosso processador quântico em um qubit lógico, esperamos reduzir as taxas de erro para permitir algoritmos quânticos úteis.\n",
    "\n",
    "TL;DR:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PATmHivJKCS"
   },
   "source": [
    "### Summarize text into bullet points\n",
    "In the following example, you use same text on quantum computing, but ask the model to summarize it in bullet-point form. Feel free to change the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2orkDF2VJKCT"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Forneça um resumo muito curto em quatro bullets para o seguinte artigo:\n",
    "\n",
    "Nossos computadores quânticos funcionam manipulando qubits de uma forma orquestrada que chamamos de algoritmos quânticos.\n",
    "O desafio é que os qubits são tão sensíveis que até mesmo a luz difusa pode causar erros de cálculo – e o problema piora à medida que os computadores quânticos crescem.\n",
    "Isso tem consequências significativas, pois os melhores algoritmos quânticos que conhecemos para executar aplicativos úteis exigem que as taxas de erro de nossos qubits sejam muito menores do que as que temos hoje.\n",
    "Para preencher essa lacuna, precisaremos de correção de erro quântica.\n",
    "A correção de erros quânticos protege as informações codificando-as em vários qubits físicos para formar um “qubit lógico” e acredita-se que seja a única maneira de produzir um computador quântico de grande escala com taxas de erro baixas o suficiente para cálculos úteis.\n",
    "Em vez de calcular nos próprios qubits individuais, calcularemos em qubits lógicos. Ao codificar números maiores de qubits físicos em nosso processador quântico em um qubit lógico, esperamos reduzir as taxas de erro para permitir algoritmos quânticos úteis.\n",
    "\n",
    "Bullets:\n",
    "\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.2, max_output_tokens=256, top_k=1, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE7y-clBJKCT"
   },
   "source": [
    "###  Dialogue summarization with to-dos\n",
    "Dialogue summarization involves condensing a conversation into a shorter format so that you don't need to read the whole discussion but can leverage a summary. In this example, you ask the model to summarize an example conversation between an online retail customer and a support agent, and include to-dos at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SV-BWzRhJKCT"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Gere um resumo da conversa a seguir e, no final, resuma as tarefas para o agente de suporte:\n",
    "\n",
    "Cliente: Olá, sou José e recebi o item errado.\n",
    "\n",
    "Agente de suporte: Olá, José. Como você gostaria de ver isso resolvido?\n",
    "\n",
    "Cliente: Eu quero devolver o item e obter um reembolso, por favor.\n",
    "\n",
    "Agente de Suporte: Claro. Posso processar o reembolso para você agora. Posso ter o número do seu pedido, por favor?\n",
    "\n",
    "Cliente: É [NÚMERO DO PEDIDO].\n",
    "\n",
    "Agente de suporte: Obrigado. Processei o reembolso e você receberá seu dinheiro de volta em 14 dias.\n",
    "\n",
    "Cliente: Muito obrigado.\n",
    "\n",
    "Agente de suporte: De nada, José. Tenha um bom dia!\n",
    "\n",
    "Resumo:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.2, max_output_tokens=256, top_k=40, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlOgWzmNJKCT"
   },
   "source": [
    "###  Hashtag tokenization\n",
    "Hashtag tokenization is the process of taking a piece of text and getting the hashtag \"tokens\" out of it. You can use this, for example, if you want to generate hashtags for your social media campaigns. In this example, you take [this tweet from Google Cloud](https://twitter.com/googlecloud/status/1649127992348606469) and generate some hashtags you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWa8rNV0JKCT"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Tokenize as hashtags deste tweet:\n",
    "\n",
    "Google Cloud\n",
    "@googlecloud\n",
    "Como os dados podem ajudar nosso planeta em mudança? 🌎\n",
    "\n",
    "Em homenagem ao #EarthDay neste fim de semana, estamos orgulhosos de compartilhar como estamos fazendo parceria com\n",
    "@ClimateEngine para aproveitar o poder dos dados geoespaciais e direcionar para um futuro mais sustentável.\n",
    "\n",
    "Confira como → https://goo.gle/3mOUfts\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    generation_model.predict(prompt, temperature=0.8, max_output_tokens=1024, top_k=40, top_p=0.8).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f-w7mUxJKCT"
   },
   "source": [
    "### Title & heading generation\n",
    "Below, you ask the model to generate five options for possible title/heading combos for a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWNri4DTJKCU"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Escreva um título para este texto, me dê cinco opções:\n",
    "Seja ajudando médicos a identificar doenças ou encontrando fotos de “abraços”, a IA está por trás de muito do trabalho que fazemos no Google. E em nosso Arts & Culture Lab em Paris, temos experimentado como a IA pode ser usada em benefício da cultura.\n",
    "Hoje, estamos compartilhando nossos últimos experimentos - protótipos que se baseiam em sete anos de trabalho em parceria com 1.500 instituições culturais em todo o mundo.\n",
    "Cada um desses aplicativos experimentais executa algoritmos de IA em segundo plano para permitir que você descubra conexões culturais escondidas em arquivos e até mesmo encontre obras de arte que combinem com a decoração da sua casa.\"\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.8, max_output_tokens=256, top_k=1, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcpmZnwKJKCU"
   },
   "source": [
    "## Evaluation\n",
    "You can evaluate the outputs from summarization tasks using [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) as an evalulation framework. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) are measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.\n",
    "\n",
    "\n",
    "The first step is to install the ROUGE library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJcl38ElJKCU"
   },
   "outputs": [],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD9eKq3SJKCU"
   },
   "source": [
    "Create a summary from a language model that you can use to compare against a human-generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37m_fb-HJKCU"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "ROUGE = Rouge()\n",
    "\n",
    "prompt = traduza(\"\"\"\n",
    "Forneça um resumo muito curto, máximo de quatro frases, para o seguinte artigo:\n",
    "\n",
    "Nossos computadores quânticos funcionam manipulando qubits de uma forma orquestrada que chamamos de algoritmos quânticos.\n",
    "O desafio é que os qubits são tão sensíveis que até mesmo a luz difusa pode causar erros de cálculo – e o problema piora à medida que os computadores quânticos crescem.\n",
    "Isso tem consequências significativas, pois os melhores algoritmos quânticos que conhecemos para executar aplicativos úteis exigem que as taxas de erro de nossos qubits sejam muito menores do que as que temos hoje.\n",
    "Para preencher essa lacuna, precisaremos de correção de erro quântica.\n",
    "A correção de erros quânticos protege as informações codificando-as em vários qubits físicos para formar um “qubit lógico” e acredita-se que seja a única maneira de produzir um computador quântico de grande escala com taxas de erro baixas o suficiente para cálculos úteis.\n",
    "Em vez de calcular nos próprios qubits individuais, calcularemos em qubits lógicos. Ao codificar números maiores de qubits físicos em nosso processador quântico em um qubit lógico, esperamos reduzir as taxas de erro para permitir algoritmos quânticos úteis.\n",
    "\n",
    "Resumo:\n",
    "\n",
    "\"\"\", \"en\")\n",
    "\n",
    "candidate = traduza(generation_model.predict(prompt, temperature=0.1, max_output_tokens=1024, top_k=40, top_p=0.9).text, \"pt\")\n",
    "\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b44f9872e1ba"
   },
   "source": [
    "You will also need a human-generated summary that we will use to compare to the `candidate` generated by the model. We will call this `reference`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0qNdPzOJKCc"
   },
   "outputs": [],
   "source": [
    "reference = \"Computadores quânticos são sensíveis a ruídos e erros. Para preencher essa lacuna, precisaremos de correção de erros quânticos. A correção de erros quânticos protege as informações codificando vários qubits físicos para formar um ”qubit lógico”.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KKaYhzwJKCc"
   },
   "source": [
    "Now you can take the candidate and reference to evaluate the performance. In this case, ROUGE will give you:\n",
    "\n",
    "- `rouge-1`, which measures unigram overlap\n",
    "- `rouge-2`, which measures bigram overlap\n",
    "- `rouge-l`, which measures the longest common subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHUH6VuTJKCc"
   },
   "outputs": [],
   "source": [
    "ROUGE.get_scores(candidate, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_summarization.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
