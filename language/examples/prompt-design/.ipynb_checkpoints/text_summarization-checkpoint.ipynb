{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Text Summarization with Generative Models on Vertex AI\n",
    "\n",
    "<table align=\"left\">\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/text_summarization.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/text_summarization.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/prompt-design/text_summarization.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "Text summarization produces a concise and fluent summary of a longer text document. There are two main text summarization types: extractive and abstractive. Extractive summarization involves selecting critical sentences from the original text and combining them to form a summary. Abstractive summarization involves generating new sentences representing the original text's main points. In this notebook, you go through a few examples of how large language models can help with generating summaries based on text.\n",
    "\n",
    "Learn more about text summarization in the [official documentation](https://cloud.google.com/vertex-ai/docs/generative-ai/text/summarization-prompts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d975e698c9a4"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you will learn how to use generative models to summarize information from text by working through the following examples:\n",
    "- Transcript summarization\n",
    "- Summarizing text into bullet points\n",
    "- Dialogue summarization with to-dos\n",
    "- Hashtag tokenization\n",
    "- Title & heading generation\n",
    "\n",
    "You also learn how to evaluate model-generated summaries by comparing to human-created summaries using ROUGE as an evaluation framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f6d865e68adb"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI Generative AI Studio\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing),\n",
    "and use the [Pricing Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bs9TZo0GJKCR"
   },
   "source": [
    "## Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2a5AEr0lkLKD"
   },
   "source": [
    "### Install Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "148dd6321946",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform google-cloud-translate --upgrade --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLVWFKFwkLKE"
   },
   "source": [
    "**Colab only:** Uncomment the following cell to restart the kernel or use the button to restart the kernel. For Vertex AI Workbench you can restart the terminal using the button on top. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Hsqwn4hkLKE"
   },
   "outputs": [],
   "source": [
    "# # Automatically restart kernel after installs so that your environment can access the new packages\n",
    "# import IPython\n",
    "\n",
    "# app = IPython.Application.instance()\n",
    "# app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xe7OuYuGkLKF"
   },
   "source": [
    "### Authenticating your notebook environment\n",
    "* If you are using **Colab** to run this notebook, uncomment the cell below and continue.\n",
    "* If you are using **Vertex AI Workbench**, check out the setup instructions [here](#TODO).\n",
    "* If you are using **local Jupyter**, check out the setup instructions [here](#TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U9Gx2SAZkLKF"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "960505627ddf"
   },
   "source": [
    "### Import libraries\n",
    "\n",
    "Let's start by importing the libraries that we will need for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Colab only:** Uncomment the following cell to initialize the Vertex AI SDK. For Vertex AI Workbench, you don't need to run this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import vertexai\n",
    "\n",
    "# PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}\n",
    "# vertexai.init(project=PROJECT_ID, location=\"us-central1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PyQmSRbKA8r-"
   },
   "outputs": [],
   "source": [
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UP76a2la7O-a"
   },
   "source": [
    "### Import models\n",
    "\n",
    "Here we load the pre-trained text generation model called `text-bison@001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7isig7e07O-a"
   },
   "outputs": [],
   "source": [
    "generation_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the translation wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import translate\n",
    "\n",
    "project_id = !gcloud config list project\n",
    "project_id = project_id[1].split('=')[1].strip()\n",
    "parent = f'projects/' + project_id\n",
    "\n",
    "\n",
    "def traduza(texto, idioma_destino):\n",
    "    client = translate.TranslationServiceClient()\n",
    "\n",
    "    response = client.translate_text(\n",
    "        parent=parent,\n",
    "        contents=[texto],\n",
    "        target_language_code=idioma_destino,\n",
    "        mime_type=\"text/plain\"\n",
    "    )\n",
    "\n",
    "    return response.translations[0].translated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mu1UAhoTKn51"
   },
   "source": [
    "## Text Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TgZvJeBpJKCS"
   },
   "source": [
    "### Transcript summarization\n",
    "\n",
    "In this first example, you summarize a piece of text on quantum computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UA2NjngeJKCS"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Forne√ßa um resumo muito curto, n√£o mais do que tr√™s frases, para o seguinte artigo:\n",
    "\n",
    "Nossos computadores qu√¢nticos funcionam manipulando qubits de uma forma orquestrada que chamamos de algoritmos qu√¢nticos.\n",
    "O desafio √© que os qubits s√£o t√£o sens√≠veis que at√© mesmo a luz difusa pode causar erros de c√°lculo ‚Äì e o problema piora √† medida que os computadores qu√¢nticos crescem.\n",
    "Isso tem consequ√™ncias significativas, pois os melhores algoritmos qu√¢nticos que conhecemos para executar aplicativos √∫teis exigem que as taxas de erro de nossos qubits sejam muito menores do que as que temos hoje.\n",
    "Para preencher essa lacuna, precisaremos de corre√ß√£o de erro qu√¢ntica.\n",
    "A corre√ß√£o de erros qu√¢nticos protege as informa√ß√µes codificando-as em v√°rios qubits f√≠sicos para formar um ‚Äúqubit l√≥gico‚Äù e acredita-se que seja a √∫nica maneira de produzir um computador qu√¢ntico de grande escala com taxas de erro baixas o suficiente para c√°lculos √∫teis.\n",
    "Em vez de calcular nos pr√≥prios qubits individuais, calcularemos em qubits l√≥gicos. Ao codificar n√∫meros maiores de qubits f√≠sicos em nosso processador qu√¢ntico em um qubit l√≥gico, esperamos reduzir as taxas de erro para permitir algoritmos qu√¢nticos √∫teis.\n",
    "\n",
    "Resumo:\n",
    "\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aade04b2e86a"
   },
   "source": [
    "Instead of a summary, we can ask for a TL;DR (\"too long; didn't read\"). You can compare the differences between the outputs generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0c0c0f3dfe10"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Forne√ßa um TL;DR para o seguinte artigo:\n",
    "\n",
    "Nossos computadores qu√¢nticos funcionam manipulando qubits de uma forma orquestrada que chamamos de algoritmos qu√¢nticos.\n",
    "O desafio √© que os qubits s√£o t√£o sens√≠veis que at√© mesmo a luz difusa pode causar erros de c√°lculo ‚Äì e o problema piora √† medida que os computadores qu√¢nticos crescem.\n",
    "Isso tem consequ√™ncias significativas, pois os melhores algoritmos qu√¢nticos que conhecemos para executar aplicativos √∫teis exigem que as taxas de erro de nossos qubits sejam muito menores do que as que temos hoje.\n",
    "Para preencher essa lacuna, precisaremos de corre√ß√£o de erro qu√¢ntica.\n",
    "A corre√ß√£o de erros qu√¢nticos protege as informa√ß√µes codificando-as em v√°rios qubits f√≠sicos para formar um ‚Äúqubit l√≥gico‚Äù e acredita-se que seja a √∫nica maneira de produzir um computador qu√¢ntico de grande escala com taxas de erro baixas o suficiente para c√°lculos √∫teis.\n",
    "Em vez de calcular nos pr√≥prios qubits individuais, calcularemos em qubits l√≥gicos. Ao codificar n√∫meros maiores de qubits f√≠sicos em nosso processador qu√¢ntico em um qubit l√≥gico, esperamos reduzir as taxas de erro para permitir algoritmos qu√¢nticos √∫teis.\n",
    "\n",
    "TL;DR:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.2, max_output_tokens=1024, top_k=40, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5PATmHivJKCS"
   },
   "source": [
    "### Summarize text into bullet points\n",
    "In the following example, you use same text on quantum computing, but ask the model to summarize it in bullet-point form. Feel free to change the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2orkDF2VJKCT"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Forne√ßa um resumo muito curto em quatro bullets para o seguinte artigo:\n",
    "\n",
    "Nossos computadores qu√¢nticos funcionam manipulando qubits de uma forma orquestrada que chamamos de algoritmos qu√¢nticos.\n",
    "O desafio √© que os qubits s√£o t√£o sens√≠veis que at√© mesmo a luz difusa pode causar erros de c√°lculo ‚Äì e o problema piora √† medida que os computadores qu√¢nticos crescem.\n",
    "Isso tem consequ√™ncias significativas, pois os melhores algoritmos qu√¢nticos que conhecemos para executar aplicativos √∫teis exigem que as taxas de erro de nossos qubits sejam muito menores do que as que temos hoje.\n",
    "Para preencher essa lacuna, precisaremos de corre√ß√£o de erro qu√¢ntica.\n",
    "A corre√ß√£o de erros qu√¢nticos protege as informa√ß√µes codificando-as em v√°rios qubits f√≠sicos para formar um ‚Äúqubit l√≥gico‚Äù e acredita-se que seja a √∫nica maneira de produzir um computador qu√¢ntico de grande escala com taxas de erro baixas o suficiente para c√°lculos √∫teis.\n",
    "Em vez de calcular nos pr√≥prios qubits individuais, calcularemos em qubits l√≥gicos. Ao codificar n√∫meros maiores de qubits f√≠sicos em nosso processador qu√¢ntico em um qubit l√≥gico, esperamos reduzir as taxas de erro para permitir algoritmos qu√¢nticos √∫teis.\n",
    "\n",
    "Bullets:\n",
    "\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.2, max_output_tokens=256, top_k=1, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yE7y-clBJKCT"
   },
   "source": [
    "###  Dialogue summarization with to-dos\n",
    "Dialogue summarization involves condensing a conversation into a shorter format so that you don't need to read the whole discussion but can leverage a summary. In this example, you ask the model to summarize an example conversation between an online retail customer and a support agent, and include to-dos at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SV-BWzRhJKCT"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Gere um resumo da conversa a seguir e, no final, resuma as tarefas para o agente de suporte:\n",
    "\n",
    "Cliente: Ol√°, sou Jos√© e recebi o item errado.\n",
    "\n",
    "Agente de suporte: Ol√°, Jos√©. Como voc√™ gostaria de ver isso resolvido?\n",
    "\n",
    "Cliente: Eu quero devolver o item e obter um reembolso, por favor.\n",
    "\n",
    "Agente de Suporte: Claro. Posso processar o reembolso para voc√™ agora. Posso ter o n√∫mero do seu pedido, por favor?\n",
    "\n",
    "Cliente: √â [N√öMERO DO PEDIDO].\n",
    "\n",
    "Agente de suporte: Obrigado. Processei o reembolso e voc√™ receber√° seu dinheiro de volta em 14 dias.\n",
    "\n",
    "Cliente: Muito obrigado.\n",
    "\n",
    "Agente de suporte: De nada, Jos√©. Tenha um bom dia!\n",
    "\n",
    "Resumo:\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.2, max_output_tokens=256, top_k=40, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlOgWzmNJKCT"
   },
   "source": [
    "###  Hashtag tokenization\n",
    "Hashtag tokenization is the process of taking a piece of text and getting the hashtag \"tokens\" out of it. You can use this, for example, if you want to generate hashtags for your social media campaigns. In this example, you take [this tweet from Google Cloud](https://twitter.com/googlecloud/status/1649127992348606469) and generate some hashtags you can use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BWa8rNV0JKCT"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Tokenize as hashtags deste tweet:\n",
    "\n",
    "Google Cloud\n",
    "@googlecloud\n",
    "Como os dados podem ajudar nosso planeta em mudan√ßa? üåé\n",
    "\n",
    "Em homenagem ao #EarthDay neste fim de semana, estamos orgulhosos de compartilhar como estamos fazendo parceria com\n",
    "@ClimateEngine para aproveitar o poder dos dados geoespaciais e direcionar para um futuro mais sustent√°vel.\n",
    "\n",
    "Confira como ‚Üí https://goo.gle/3mOUfts\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    generation_model.predict(prompt, temperature=0.8, max_output_tokens=1024, top_k=40, top_p=0.8).text\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7f-w7mUxJKCT"
   },
   "source": [
    "### Title & heading generation\n",
    "Below, you ask the model to generate five options for possible title/heading combos for a given piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MWNri4DTJKCU"
   },
   "outputs": [],
   "source": [
    "prompt = traduza(\"\"\"\n",
    "Escreva um t√≠tulo para este texto, me d√™ cinco op√ß√µes:\n",
    "Seja ajudando m√©dicos a identificar doen√ßas ou encontrando fotos de ‚Äúabra√ßos‚Äù, a IA est√° por tr√°s de muito do trabalho que fazemos no Google. E em nosso Arts & Culture Lab em Paris, temos experimentado como a IA pode ser usada em benef√≠cio da cultura.\n",
    "Hoje, estamos compartilhando nossos √∫ltimos experimentos - prot√≥tipos que se baseiam em sete anos de trabalho em parceria com 1.500 institui√ß√µes culturais em todo o mundo.\n",
    "Cada um desses aplicativos experimentais executa algoritmos de IA em segundo plano para permitir que voc√™ descubra conex√µes culturais escondidas em arquivos e at√© mesmo encontre obras de arte que combinem com a decora√ß√£o da sua casa.\"\n",
    "\"\"\", \"en\")\n",
    "\n",
    "print(\n",
    "    traduza(generation_model.predict(prompt, temperature=0.8, max_output_tokens=256, top_k=1, top_p=0.8).text, \"pt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zcpmZnwKJKCU"
   },
   "source": [
    "## Evaluation\n",
    "You can evaluate the outputs from summarization tasks using [ROUGE](https://en.wikipedia.org/wiki/ROUGE_(metric)) as an evalulation framework. ROUGE (Recall-Oriented Understudy for Gisting Evaluation) are measures to automatically determine the quality of a summary by comparing it to other (ideal) summaries created by humans. The measures count the number of overlapping units such as n-gram, word sequences, and word pairs between the computer-generated summary to be evaluated and the ideal summaries created by humans.\n",
    "\n",
    "\n",
    "The first step is to install the ROUGE library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tJcl38ElJKCU"
   },
   "outputs": [],
   "source": [
    "!pip install rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD9eKq3SJKCU"
   },
   "source": [
    "Create a summary from a language model that you can use to compare against a human-generated summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37m_fb-HJKCU"
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge\n",
    "\n",
    "ROUGE = Rouge()\n",
    "\n",
    "prompt = traduza(\"\"\"\n",
    "Forne√ßa um resumo muito curto, m√°ximo de quatro frases, para o seguinte artigo:\n",
    "\n",
    "Nossos computadores qu√¢nticos funcionam manipulando qubits de uma forma orquestrada que chamamos de algoritmos qu√¢nticos.\n",
    "O desafio √© que os qubits s√£o t√£o sens√≠veis que at√© mesmo a luz difusa pode causar erros de c√°lculo ‚Äì e o problema piora √† medida que os computadores qu√¢nticos crescem.\n",
    "Isso tem consequ√™ncias significativas, pois os melhores algoritmos qu√¢nticos que conhecemos para executar aplicativos √∫teis exigem que as taxas de erro de nossos qubits sejam muito menores do que as que temos hoje.\n",
    "Para preencher essa lacuna, precisaremos de corre√ß√£o de erro qu√¢ntica.\n",
    "A corre√ß√£o de erros qu√¢nticos protege as informa√ß√µes codificando-as em v√°rios qubits f√≠sicos para formar um ‚Äúqubit l√≥gico‚Äù e acredita-se que seja a √∫nica maneira de produzir um computador qu√¢ntico de grande escala com taxas de erro baixas o suficiente para c√°lculos √∫teis.\n",
    "Em vez de calcular nos pr√≥prios qubits individuais, calcularemos em qubits l√≥gicos. Ao codificar n√∫meros maiores de qubits f√≠sicos em nosso processador qu√¢ntico em um qubit l√≥gico, esperamos reduzir as taxas de erro para permitir algoritmos qu√¢nticos √∫teis.\n",
    "\n",
    "Resumo:\n",
    "\n",
    "\"\"\", \"en\")\n",
    "\n",
    "candidate = traduza(generation_model.predict(prompt, temperature=0.1, max_output_tokens=1024, top_k=40, top_p=0.9).text, \"pt\")\n",
    "\n",
    "print(candidate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b44f9872e1ba"
   },
   "source": [
    "You will also need a human-generated summary that we will use to compare to the `candidate` generated by the model. We will call this `reference`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D0qNdPzOJKCc"
   },
   "outputs": [],
   "source": [
    "reference = \"Computadores qu√¢nticos s√£o sens√≠veis a ru√≠dos e erros. Para preencher essa lacuna, precisaremos de corre√ß√£o de erros qu√¢nticos. A corre√ß√£o de erros qu√¢nticos protege as informa√ß√µes codificando v√°rios qubits f√≠sicos para formar um ‚Äùqubit l√≥gico‚Äù.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1KKaYhzwJKCc"
   },
   "source": [
    "Now you can take the candidate and reference to evaluate the performance. In this case, ROUGE will give you:\n",
    "\n",
    "- `rouge-1`, which measures unigram overlap\n",
    "- `rouge-2`, which measures bigram overlap\n",
    "- `rouge-l`, which measures the longest common subsequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FHUH6VuTJKCc"
   },
   "outputs": [],
   "source": [
    "ROUGE.get_scores(candidate, reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "text_summarization.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m107",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m107"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
